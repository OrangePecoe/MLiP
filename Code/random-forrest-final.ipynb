{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Import widgets\nfrom ipywidgets import widgets, interactive, interact\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\ncalendar_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nsubmission_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\nprices_df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ## Calculating WRMSSEE, from: https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834"},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\n\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fold_df = sales_df.iloc[:, :-56]\nvalid_fold_df = sales_df.iloc[:, -28:]\nevaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar_df, prices_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualizing data series for different splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = range(1 , 1941 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\n\nids = np.random.choice(sales_df['id'].unique().tolist(), 1000)\n\nseries_ids = widgets.Dropdown(\n    options=ids,\n    value=ids[0],\n    description='series_ids:'\n)\n\ndef plot_data(series_ids):\n    df = sales_df.loc[sales_df['id'] == series_ids][time_series_columns]\n    df = pd.Series(df.values.flatten())\n\n    df.plot(figsize=(20, 10), lw=2, marker='*')\n    df.rolling(7).mean().plot(figsize=(20, 10), lw=2, marker='o', color='orange')\n    plt.axhline(df.mean(), lw=3, color='red')\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = interactive(\n    plot_data,\n    series_ids=series_ids\n)\ndisplay(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_values1 = sales_df[['store_id']].values.ravel()\nstores = pd.unique(column_values1)\n\ncolumn_values1 = sales_df[['dept_id']].values.ravel()\ndepartments = pd.unique(column_values1)\n\nprint(stores)\nprint(departments)\nfor i in stores:\n    \n    df = sales_df.loc[(sales_df['dept_id'] == 'HOUSEHOLD_2') & (sales_df['store_id'] == i)][time_series_columns]\n    \n    df2 = df.sum()\n    df2 = pd.Series(df2.values.flatten())\n    \n    ax = df2.rolling(7).mean().plot(figsize=(20, 10), lw=2)\n    ax.set_xlabel(\"days passed\")\n    ax.set_ylabel(\"sum of sales\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in departments:\n    \n    df = sales_df.loc[(sales_df['dept_id'] == i)][time_series_columns]\n    df2 = df.sum()\n    df2 = pd.Series(df2.values.flatten())\n    df2.rolling(7).mean().plot(figsize=(20, 10), lw=2)\n    \n    ax.set_xlabel(\"days passed\")\n    ax.set_ylabel(\"sum of sales\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## changing datatypes, from: https://www.kaggle.com/ar2017/m5-forecasting-lightgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correct data types for \"calendar.csv\"\ncalendarDTypes = {\"event_name_1\": \"category\", \n                  \"event_name_2\": \"category\", \n                  \"event_type_1\": \"category\", \n                  \"event_type_2\": \"category\", \n                  \"weekday\": \"category\", \n                  'wm_yr_wk': 'int16', \n                  \"wday\": \"int16\",\n                  \"month\": \"int16\", \n                  \"year\": \"int16\", \n                  \"snap_CA\": \"float32\", \n                  'snap_TX': 'float32', \n                  'snap_WI': 'float32' }\n\n# Read csv file\ncalendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", \n                       dtype = calendarDTypes)\n\ncalendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n\n# Transform categorical features into integers\nfor col, colDType in calendarDTypes.items():\n    if colDType == \"category\":\n        calendar[col] = calendar[col].cat.codes.astype(\"int16\")\n        calendar[col] -= calendar[col].min()\n\ncalendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"firstDay = 1\nlastDay = 1941\n\n# Use x sales days (columns) for training\nnumCols = [f\"d_{day}\" for day in range(firstDay, lastDay+1)]\n\n# Define all categorical columns\ncatCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n\n# Define the correct data types for \"sales_train_validation.csv\"\ndtype = {numCol: \"float32\" for numCol in numCols} \ndtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n\n# Read csv file\nsales = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", \n                 usecols = catCols + numCols, dtype = dtype)\n\n# Transform categorical features into integers\nfor col in catCols:\n    if col != \"id\":\n        sales[col] = sales[col].cat.codes.astype(\"int16\")\n        sales[col] -= sales[col].min()\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeatures = calendar[{'year','snap_TX','snap_CA','wday','month', 'snap_WI', \"event_name_1\",\"event_type_1\", \"event_name_2\",\"event_type_2\"}].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for each time series, take random indices. for each indices, take previous 14 days and calander data of day 15 as featuers and day 15 as y. \n# now this is done for each dept_id and each store, but can also be done for diferent splits. \n\ncolumn_values1 = sales[['store_id']].values.ravel()\nstores = pd.unique(column_values1)\n\ncolumn_values1 = sales[['dept_id']].values.ravel()\ndepartments = pd.unique(column_values1)\n\ndays = range(1, 1941 + 1)\n\nfirst_day = 1\n\ntime_series_columns = [f'd_{i}' for i in days]\n\n\npredict_validation = np.zeros((30490, 28))\npredict_evaluation = np.zeros((30490, 28))\n\n\nfor storeid in stores:\n    for deptid in departments:\n\n\n        itemseries2 = sales.loc[(sales['dept_id'] == deptid ) & (sales['store_id'] == storeid)]\n        indexen = itemseries2.index.values.tolist()\n        itemseries = itemseries2[time_series_columns].values\n\n\n\n        n1 = 100 #number of samplse per time series\n        xo =  1\n        xi = 14\n        train_x_data = np.zeros((len(itemseries)* n1,xi+len(features[0])))\n        train_y_data =  np.zeros(len(train_x_data))\n\n        # for loop creating the samples.\n        for i in range(len(itemseries)):\n            x = itemseries[i]\n\n            n_random = np.random.choice(  np.arange(first_day + 60 ,len(x)-84), n1, replace = False)\n        \n            x_train = np.zeros((n1,xi+len(features[0])))\n            y_train = np.zeros(len(x_train))\n\n            for k, j in enumerate(n_random):\n                x_train[k] = np.concatenate((x[j -xi-xo:j-1], features[j-1]), axis = 0) \n                y_train[k] = x[j-1]\n\n            train_x_data[i*n1:i*n1+n1] = x_train\n            train_y_data[i*n1:i*n1+n1] = y_train\n\n        rf = RandomForestRegressor(n_estimators = 100, random_state = 4)# Train the model on training data\n        rf.fit(train_x_data,train_y_data )\n\n\n\n\n\n        ## predicting the validation data: days 1914 - 1941\n\n        x_predict = itemseries[:,-14-28:-28] #initial last 14 days for day 1914\n\n        predict_features = features[-56:-28] # calander data of the 28 days to predict\n\n        # concatenating the above 2 variables\n        predict_total = np.zeros((len(x_predict), len(x_predict[0])+len(predict_features[0])))\n        for z in range(len(x_predict)):\n            predict_total[z] = np.concatenate((x_predict[z],predict_features[0]), axis= 0)\n\n        #predicting the first day for every time series\n        initial_predict = rf.predict(predict_total)\n\n\n        #this for loop takes the previous last 13 days of a predicion and add the prediction of the previous day as the 14th day. \n        # it then adds the calandar data of that day as well.\n        # the prediction is added to predict_total2 and then the loop is repeated. \n        predict_total2 = np.zeros((len(x_predict),28))\n        temp_prediction = initial_predict\n        predict_total2[:,0] = temp_prediction\n        to_predict = x_predict.astype(np.float32)\n        for i in range(1,28):\n           \n            to_predict[:,:13] = to_predict[:,-13:]\n            to_predict[:,13] = temp_prediction\n\n            predict_total = np.zeros((len(x_predict), len(x_predict[0])+len(predict_features[0])))\n            for j in range(len(x_predict)):\n                predict_total[j] = np.concatenate((to_predict[j],predict_features[i]), axis= 0)\n\n            temp_prediction = rf.predict(predict_total)\n            predict_total2[:,i] = temp_prediction\n\n       \n        predict_validation[indexen] = predict_total2\n\n\n    \n    \n    \n    ## predicting the evaluation data: days 1942 - 1969\n    \n    x_predict = itemseries[:,-14:] #initial last 14 days for day 1914\n    \n    predict_features = features[-28:] # calander data of the 28 days to predict\n\n    # concatenating the above 2 variables\n    predict_total = np.zeros((len(x_predict), len(x_predict[0])+len(predict_features[0])))\n    for z in range(len(x_predict)):\n        predict_total[z] = np.concatenate((x_predict[z],predict_features[0]), axis= 0)\n\n    #predictin the first day for every time series\n    initial_predict = rf.predict(predict_total)\n\n\n    #this forloop takes the previous last 13 days of a predicion and add the prediction of the previous day as the 14th day. \n    # it then adds the calandar data of that day as well.\n    # the prediction is added to predict_total2 and then the loop is repeated. \n    predict_total2 = np.zeros((len(x_predict),28))\n    temp_prediction = initial_predict\n    predict_total2[:,0] = temp_prediction\n    to_predict = x_predict.astype(np.float32)\n    for i in range(1,28):\n\n        to_predict[:,:13] = to_predict[:,-13:]\n        to_predict[:,13] = temp_prediction\n\n        predict_total = np.zeros((len(x_predict), len(x_predict[0])+len(predict_features[0])))\n        for j in range(len(x_predict)):\n            predict_total[j] = np.concatenate((to_predict[j],predict_features[i]), axis= 0)\n\n  \n        temp_prediction = rf.predict(predict_total)\n        predict_total2[:,i] = temp_prediction\n  \n    predict_evaluation[indexen] = predict_total2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## next cell was used if rolling means and lags were the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\n#for each time series, take 10 random indices. for each indices, take previous 14 days and calander data of day 15 as featuers and day 15 as y. \n# now this is done for each dept_id\n\n#for itemid in unique_values1:\n#    itemseries = train_sales.loc[(train_sales['dept_id'] == itemid)]\n#    print(len(itemseries))\n\ncolumn_values1 = sales[['store_id']].values.ravel()\nstores = pd.unique(column_values1)\n\ncolumn_values1 = sales[['dept_id']].values.ravel()\ndepartments = pd.unique(column_values1)\n\n\n\ndays = range(1, 1941 + 1)\nval_days = range(1914, 1941+1)\nfirst_day = 1\n\ntime_series_columns = [f'd_{i}' for i in days]\nval_days_columns = [f'd_{i}' for i in val_days]\n\n# shape is number of series, number of predicted days\npredict_total3 = np.zeros((30490, 56))\n\n\n\n#for storeid in stores:\nfor deptid in departments:\n    \n\n    itemseries2 = sales.loc[(sales['dept_id'] == deptid)] # & (sales['store_id'] == storeid)\n    indexen = itemseries2.index.values.tolist()\n    itemseries = itemseries2[time_series_columns].values\n\n\n\n    n1 = 100 #100  #number of samplse per time series\n    xo =  1\n    xi = 6\n    train_x_data = np.zeros((len(itemseries)* n1,xi+len(features[0])))\n    train_y_data =  np.zeros(len(train_x_data))\n\n\n    for i in range(len(itemseries)):\n        x = itemseries[i]\n\n        \n        n_random = np.random.choice( np.arange(first_day + 60 ,len(x)-84), n1, replace = False)\n\n        x_train = np.zeros((n1,xi+len(features[0])))\n        y_train = np.zeros(len(x_train))\n\n        #creating training  samples\n        for k, j in enumerate(n_random):\n            lag_7 = x[j-8]\n            lag_28 = x[j-29]\n            rmean_7_7 = np.mean(x[j-15:j-8])\n            rmean_28_7 = np.mean(x[j-36:j-29])\n            rmean_7_28 = np.mean(x[j-36:j-8])\n            rmean_28_28 = np.mean(x[j-57:j-29])\n            means = np.array([lag_7,lag_28,rmean_7_7,rmean_28_7,rmean_7_28,rmean_28_28])\n            x_train[k] = np.concatenate((means, features[j-1]), axis = 0) \n            y_train[k] = x[j-1]\n       \n\n\n        train_x_data[i*n1:i*n1+n1] = x_train\n        train_y_data[i*n1:i*n1+n1] = y_train\n\n    rf = RandomForestRegressor(n_estimators = 100, random_state = 4)# Train the model on training data\n    rf.fit(train_x_data,train_y_data )\n\n    predict_features = features[-56:] # calander data of the 56 days to predict\n\n\n    predict_total2 = np.zeros((len(itemseries),56))\n\n\n    k = itemseries[:,:-28] \n    q = itemseries\n\n    \n    #making predictions for 56 days (28 validation and 28 for submission)\n    for i in range(56):\n        if i < 28:\n            \n\n\n            lag_7 = k[:,-7]\n            lag_28 = k[:,-28]\n            rmean_7_7 = np.mean(k[:,-14:-7],axis=1)\n            rmean_28_7 = np.mean(k[:,-35:-28],axis=1)\n            rmean_7_28 = np.mean(k[:,-35:-7],axis=1)\n            rmean_28_28 = np.mean(k[:,-56:-28],axis=1)\n            means = np.column_stack((lag_7,lag_28,rmean_7_7,rmean_28_7,rmean_7_28,rmean_28_28)) #np.array([lag_7,lag_28,rmean_7_7,rmean_28_7,rmean_7_28,rmean_28_28])\n\n            predict_total = np.zeros((len(itemseries), len(means[0])+len(predict_features[0])))\n            for z in range(len(itemseries)):\n                predict_total[z] = np.concatenate((means[z],predict_features[i]), axis= 0)\n\n            predict = rf.predict(predict_total)\n            k = np.column_stack((k, predict))\n          \n            predict_total2[:,i] = predict\n\n        else:\n\n            lag_7 = q[:,-7]\n            lag_28 = q[:,-28]\n            rmean_7_7 = np.mean(q[:,-14:-7],axis=1)\n            rmean_28_7 = np.mean(q[:,-35:-28],axis=1)\n            rmean_7_28 = np.mean(q[:,-35:-7],axis=1)\n            rmean_28_28 = np.mean(q[:,-56:-28],axis=1)\n            means = np.column_stack((lag_7,lag_28,rmean_7_7,rmean_28_7,rmean_7_28,rmean_28_28)) #np.array([lag_7,lag_28,rmean_7_7,rmean_28_7,rmean_7_28,rmean_28_28])\n\n            predict_total = np.zeros((len(itemseries), len(means[0])+len(predict_features[0])))\n            for z in range(len(itemseries)):\n                predict_total[z] = np.concatenate((means[z],predict_features[i]), axis= 0)\n\n            predict = rf.predict(predict_total)\n            q = np.column_stack((q, predict))\n    \n            predict_total2[:,i] = predict\n\n    predict_total3[indexen] = predict_total2\n\n\n\n\npredict_validation = predict_total3[:,:-28]\npredict_evaluation = predict_total3[:,-28:]\n\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WRMSSE on validation set (1914-1941)"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_days = range(1914, 1941+1)\nval_days_columns = [f'd_{i}' for i in val_days]\n\nvalid_preds2 = pd.DataFrame(predict_validation ,columns = val_days_columns)\n\n# getting error of 28 validation days\nprint(evaluator.score(valid_preds2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_val = pd.DataFrame(predict_validation)\nforecast_eval = pd.DataFrame(predict_evaluation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_val.columns = [f'F{i}' for i in range(1, forecast_val.shape[1] + 1)]\nforecast_eval.columns = [f'F{i}' for i in range(1, forecast_eval.shape[1] + 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_ids  = sales_df['id'].values\nvalidation_ids = [i.replace('evaluation', 'validation') for i in evaluation_ids]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = np.concatenate([validation_ids, evaluation_ids])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame(ids, columns=['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = pd.concat([forecast_val, forecast_eval]).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rf =  pd.concat([predictions, forecast], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}