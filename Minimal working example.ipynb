{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # plotting \nimport matplotlib.font_manager as fm # to plot the font\nfrom tqdm.auto import tqdm # see progress bar\nimport PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont # Darw picture from font\nfrom skimage.transform import resize # Resizing of image\nfrom cv2 import resize as cv2_resize # resizng of image\nfrom keras.preprocessing.image import ImageDataGenerator  # image augmentation on training images ONLY\nfrom sklearn.model_selection import train_test_split  # splitting the data\nimport keras.backend as K # for custom metrices implementations and other processes that we define\nfrom keras.layers import Dense,BatchNormalization,Input,Dropout,Conv2D,Flatten,MaxPool2D,LeakyReLU # keras layers\nfrom keras.models import Model #Model class\nfrom keras.optimizers import Adam #optimizer\nfrom keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping \n# Call backs acts like milestones and if/else while model is being trained\nimport gc # garbage collector\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\n/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\n/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\n/kaggle/input/bengaliai-cv19/train.csv\n/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\n/kaggle/input/bengaliai-cv19/class_map.csv\n/kaggle/input/bengaliai-cv19/train_image_data_3.parquet\n/kaggle/input/bengaliai-cv19/train_image_data_2.parquet\n/kaggle/input/bengaliai-cv19/test.csv\n/kaggle/input/bengaliai-cv19/sample_submission.csv\n/kaggle/input/bengaliai-cv19/train_image_data_1.parquet\n/kaggle/input/bengaliai-cv19/train_image_data_0.parquet\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Inputting data"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_IMG_PATH = '../input/bengaliai-cv19/train_image_data_' # just for training data\nTEST_IMG_PATH = '../input/bengaliai-cv19/test_image_data_'\nFILE_TYPE = '.parquet'\nfinal_w = final_h = 64#92 \n# these are the final dimensions of an image. We resize the image so that it fits in out memory and uses less\n# computational power. You can surely try different dimensions. There is not BEST size\n\nBATCH_SIZE = 128\n\ntrain_classes = pd.read_csv(\"../input/bengaliai-cv19/train.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape = (final_w, final_h, 1)) \n# Input layer just takes into account of the size of the input \n\nmodel = Conv2D(filters=64, kernel_size=(5,5), padding='SAME', activation='relu')(inputs) \n\n\n# These layers are required\nmodel = Flatten()(model)\ndense = Dense(192, activation = \"relu\")(model) \n\nout1 = Dense(168, activation = 'softmax',name='out_1')(dense) # names of output layers. We need these names\nout2 = Dense(11, activation = 'softmax',name='out_2')(dense)  # as they act as the keys for mapping output\nout3 = Dense(7, activation = 'softmax',name='out_3')(dense)   # to each later. See in the model.fit()\n\nmodel = Model(inputs=inputs, outputs=[out1,out2,out3]) # final line of our model construction code\n# tell the system only the start and end, it'll find the path","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_LR_P = ReduceLROnPlateau(monitor='val_out_1_loss',patience=3,verbose=1,factor=0.5,min_lr=0.00001)\n# if validation loss of out_1 is not decreasing for 3 consecutive epochs, decrease the learning rate by 0.5 \n# given if learning rate is above 0.00001 and give us little insight on what has happened verbose=1\n\nES = EarlyStopping(monitor='val_loss',patience=4, min_delta=0.0025,restore_best_weights=True)\n# stop the model from fitting data if validation loss has not decreased by 0.0025 in the last 5 epochs and \n# restore best weights for the next time\n\nMCP = ModelCheckpoint('best_model_weight.h5', monitor ='val_loss', verbose =1, \n                      save_best_only = True, save_weights_only=True)\n# save the weights in a file name specified only if the validation loss of out_1 layer has improved from \n# last save. out_1 because it's recall matters twice \n\ncallbacks = [R_LR_P,ES,MCP]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize_image(df_in,curr_w,curr_h,res_w,res_h,lib='cv2'):\n    resized = {} # empty dictonary which stores the values of pixels for each image\n    if lib=='skimage':\n        for i in range(df_in.shape[0]): # iterate through all the images in dataframe\n            image = resize(df_in.loc[df.index[i]].values.reshape(curr_w,curr_h),(res_w,res_h,1))\n            # apply resize transformations on per-row basis \n            resized[df_in.index[i]] = image.reshape(-1)  # reshape accordingly\n        resized = pd.DataFrame(resized).T # resizing swaps the rows to columns so Transpose sets to default\n    \n    else: \n        for i in range(df_in.shape[0]):\n            image = cv2_resize(df_in.loc[df.index[i]].values.reshape(curr_w,curr_h),(res_w,res_h))\n            resized[df_in.index[i]] = image.reshape(-1)\n        resized = pd.DataFrame(resized).T \n    return resized","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataGenerator(ImageDataGenerator):\n    def flow(self,x,y=None,batch_size=BATCH_SIZE,shuffle=True,sample_weight=None,seed=None,save_to_dir=None,\n             save_prefix='',save_format='png',subset=None): \n        labels_array = None # all the labels array will be concatenated in this single array\n        key_lengths = {} \n        # define a dict which maps the 'key' (y1,y2 etc) to lengths of corresponding label_array\n        ordered_labels = [] # to store the ordering in which the labels Y were passed in this class\n        for key, label_value in y.items():\n            if labels_array is None:\n                labels_array = label_value \n                # for the first time loop, it's empty, so insert first element\n            else:\n                labels_array = np.concatenate((labels_array, label_value), axis=1) \n                # concat each array of y_labels \n                \n            key_lengths[key] = label_value.shape[1] \n            ordered_labels.append(key)\n\n        for x_out, y_out in super().flow(x, labels_array, batch_size=batch_size):\n            label_dict = {} # final dictonary that'll be yielded\n            i = 0 # keeps count of the ordering of the labels and their lengths\n            for label in ordered_labels:\n                target_length = key_lengths[label]\n                label_dict[label] = y_out[:, i: i + target_length] \n                i += target_length\n\n            yield x_out, label_dict","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame = pd.DataFrame({'-1':['File -1']}) # just a random dataframe nothing much\ndrop_cols = ['image_id','grapheme_root','vowel_diacritic','consonant_diacritic','grapheme'] \n# columns to be dropped after merging with dataframe so that we can have only numerical pixel values\n\nj=-1\n#for i in tqdm(range(0,4)):\nfor i in tqdm(range(0,1)):\n    j+=1\n    gc.collect() # calling garbage collector to free up memory from last delete\n    \n    print('Importing file %s'%j)\n    df = pd.merge(pd.read_parquet(TRAIN_IMG_PATH+str(j)+FILE_TYPE),train_classes,on='image_id')\n    \n    grapheme = pd.get_dummies(df['grapheme_root']).values # get the numerical values of classes and convert\n    vowel = pd.get_dummies(df['vowel_diacritic']).values  # them to categorical so that [0,1,2] becomes \n    consonant = pd.get_dummies(df['consonant_diacritic']).values # [[1,0,0],[0,1,0],[0,0,1]] where the size of the\n    \n    print('Dropping Columns.....')\n    df.drop(drop_cols,axis=1,inplace=True) # remove all the columnsthat are not of use\n    \n    print('Resizing....')\n    df = (resize_image(df_in=df,curr_w=137,curr_h=236,\n                           res_w=final_w,res_h=final_h)/255.).astype('float32')\n    \n    print('Reshaping into Rank-4..... ')\n    df = df.values.reshape(-1, final_w,final_h,1) # reshape into rank-4 matrix \n    \n    print('Splitting Train-Test...')\n    df, x_test, y_train_graph, y_test_graph, y_train_vowel, y_test_vowel, y_train_conso, y_test_conso =\\\n    train_test_split(df, grapheme,vowel,consonant, test_size=0.15, random_state=13)\n    \n    del grapheme\n    del vowel\n    del consonant\n    gc.collect()\n    \n    print('Fitting into Generator.....')\n    aug_generator = CustomDataGenerator(featurewise_center=False,samplewise_center=False,\n                                        featurewise_std_normalization=False,samplewise_std_normalization=False,\n                                        zca_whitening=False,rotation_range=13, zoom_range = 0.17,\n                                        width_shift_range=0.18,height_shift_range=0.16,horizontal_flip=False,\n                                        vertical_flip=False) \n    \n    aug_generator.fit(df)\n    # this will JUST calculate parameters required (PCA, ZCA and others if True) no transformations performed\n    \n    print('Starting Training........')\n    history = model.fit_generator(aug_generator.flow(df, {'out_1': y_train_graph, 'out_2': y_train_vowel, \n                                                     'out_3': y_train_conso}, batch_size=BATCH_SIZE),\n                              epochs = 4, validation_data = (x_test, [y_test_graph, y_test_vowel, y_test_conso]),\n                              steps_per_epoch=df.shape[0] //128 ,callbacks=callbacks)\n    \n    stopped_at = ES.stopped_epoch # gives you >=1 at which epoch model stopped due to early stopping\n    \n    del df\n    del x_test\n    del y_train_graph\n    del y_test_graph\n    del y_train_vowel\n    del y_test_vowel\n    del y_train_conso\n    del y_test_conso\n    frame[j]='Files completed is'+str(j) # nothing\n    frame.to_csv('files.csv') # saving nothing is like committing when offline and preparing a log if crash happens\n    gc.collect()","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54b67eb83b848339023e6178c50ffb3"}},"metadata":{}},{"output_type":"stream","text":"Importing file 0\nDropping Columns.....\nResizing....\nReshaping into Rank-4..... \nSplitting Train-Test...\nFitting into Generator.....\nStarting Training........\nEpoch 1/4\n333/333 [==============================] - 33s 99ms/step - loss: 8.9146 - out_1_loss: 5.0456 - out_2_loss: 2.4495 - out_3_loss: 1.4195 - out_1_accuracy: 0.0256 - out_2_accuracy: 0.2066 - out_3_accuracy: 0.6170 - val_loss: 7.9770 - val_out_1_loss: 4.7074 - val_out_2_loss: 2.0870 - val_out_3_loss: 1.1822 - val_out_1_accuracy: 0.0256 - val_out_2_accuracy: 0.2621 - val_out_3_accuracy: 0.6293\n\nEpoch 00001: val_loss improved from inf to 7.97702, saving model to best_model_weight.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('best_model_weight.h5') \npreds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}\n\ncomponents = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\nfor j in range(4):\n    print('Processing file %s'%j)\n    df = pd.read_parquet(TEST_IMG_PATH+str(j)+FILE_TYPE)\n    df.set_index('image_id', inplace=True)\n    index_values = df.index.values\n\n    df = (resize_image(df_in=df,curr_w=137,curr_h=236,\n                           res_w=final_w,res_h=final_h)/255.).astype('float32')\n    df = df.values.reshape(-1, final_w, final_h, 1)\n    \n    preds = model.predict(df)\n\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k,id in enumerate(index_values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    \n    del df\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Writing submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = pd.DataFrame({'row_id': row_id,'target':target},columns = ['row_id','target'])\n#submission.to_csv('submission.csv',index=False)\n#submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}